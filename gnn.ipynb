{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7b7786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, numpy as np, pandas as pd, torch\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.models import DeepGraphInfomax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e37ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED    = 7\n",
    "N       = 80   # số user\n",
    "K       = 6    # số AP Wi-Fi\n",
    "Fs       = 10   # số băng con\n",
    "AREA    = 500.0  # m, vùng vuông [0,L]x[0,L]\n",
    "NOISE_W = 1e-13  # W\n",
    "PMIN, PMAX = 50e-3, 200e-3  # W\n",
    "SL, SW = 1.2, 1.0           # hệ số hài lòng (chỉ dùng trong gen)\n",
    "PRICE_C = 1.0               # giá cước LTE (chỉ dùng trong gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "50440655",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c4dd1",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cea9be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScalerNP:\n",
    "    def __init__(self): self.mean_, self.std_ = None, None\n",
    "    def fit(self, X): \n",
    "        self.mean_ = X.mean(0); self.std_ = X.std(0); self.std_[self.std_==0] = 1.0\n",
    "    def transform(self, X): return (X - self.mean_) / self.std_\n",
    "    def fit_transform(self, X): self.fit(X); return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0f6c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathloss_gain(d_m: np.ndarray, pl_ref_gain_db=-30.0, n=3.2, d0=1.0):\n",
    "    \"\"\"Return |h|^2 large-scale: PL(dB)=PL(d0)+10n log10(d/d0); gain_lin=10^(-PL/10)\"\"\"\n",
    "    d = np.maximum(d_m, 1.0)\n",
    "    PL_dB = pl_ref_gain_db + 10.0*n*np.log10(d/d0)\n",
    "    return 10**(-PL_dB/10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cc91559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(N=80, K=6, F=10, L=500.0, noise=1e-13,\n",
    "                     pmin=50e-3, pmax=200e-3, SL=1.2, SW=1.0, c=1.0, seed=7):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # vị trí\n",
    "    users = rng.uniform(0, L, (N,2))\n",
    "    aps   = rng.uniform(0, L, (K,2))\n",
    "    bs    = np.array([L/2, L/2])\n",
    "\n",
    "    # kênh lớn + Rayleigh\n",
    "    d_iL  = np.linalg.norm(users - bs, axis=1)\n",
    "    g_iL  = pathloss_gain(d_iL) * rng.exponential(1.0, N)\n",
    "    d_ik  = np.linalg.norm(users[:,None,:] - aps[None,:,:], axis=2)\n",
    "    g_ik  = pathloss_gain(d_ik) * rng.exponential(1.0, (N,K))\n",
    "\n",
    "    # tham số user\n",
    "    Ci   = rng.uniform(0.0, 8.0, N)\n",
    "    Lthr = rng.uniform(0.3, 0.8, N)\n",
    "    Pmax = rng.uniform(pmin, pmax, N)\n",
    "    P    = rng.uniform(0.5, 1.0, N) * Pmax\n",
    "\n",
    "    # gán LTE/Wi-Fi (heuristic affordability + utility)\n",
    "    snr_L = (0.5*(pmin+pmax))*g_iL/noise\n",
    "    snr_W_best = (0.5*(pmin+pmax))*np.max(g_ik, axis=1)/noise\n",
    "    uL = SL*np.log2(1+snr_L)\n",
    "    uW = SW*np.log2(1+snr_W_best)\n",
    "    x_LTE = (uL>=uW) & (c*np.log2(1+snr_L) <= Ci)\n",
    "    ap_idx = np.argmax(g_ik, axis=1)\n",
    "    ap_idx[x_LTE] = -1\n",
    "\n",
    "    # subband greedy theo giảm nhiễu\n",
    "    Psi = np.zeros((N,F), dtype=int)\n",
    "    # hbar_ij: kênh từ j -> máy thu của i\n",
    "    hbar = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        if x_LTE[i]: hbar[i,:] = g_iL[:]          # j -> BS\n",
    "        else:        hbar[i,:] = g_ik[:, ap_idx[i]]\n",
    "    np.fill_diagonal(hbar, 0.0)\n",
    "\n",
    "    users_on_f = [[] for _ in range(F)]\n",
    "    order = rng.permutation(N)\n",
    "    for i in order:\n",
    "        best_f, best_den = None, None\n",
    "        for f in range(F):\n",
    "            js = users_on_f[f]\n",
    "            den = (P[js]*hbar[i,js]).sum() + noise\n",
    "            if best_den is None or den < best_den:\n",
    "                best_den, best_f = den, f\n",
    "        Psi[i,best_f] = 1\n",
    "        users_on_f[best_f].append(i)\n",
    "\n",
    "    # SINR & rate (để kiểm soát chất lượng dữ liệu; không dùng làm label)\n",
    "    h_eff = np.where(x_LTE, g_iL, g_ik[np.arange(N), np.maximum(ap_idx,0)])\n",
    "    SINR = np.zeros(N)\n",
    "    for f in range(F):\n",
    "        ids = np.where(Psi[:,f]==1)[0]\n",
    "        if ids.size==0: continue\n",
    "        den = hbar[np.ix_(ids,ids)].dot(P[ids]) + noise\n",
    "        num = P[ids]*h_eff[ids]\n",
    "        SINR[ids] += num/den\n",
    "    rate = np.log2(1+SINR)\n",
    "\n",
    "    # DataFrame users\n",
    "    df = pd.DataFrame({\n",
    "        \"user_id\": np.arange(N),\n",
    "        \"x_LTE\": x_LTE.astype(int),\n",
    "        \"ap_idx\": ap_idx.astype(int),\n",
    "        \"subband\": Psi.argmax(1).astype(int),\n",
    "        \"P_max_W\": Pmax, \"P_W\": P, \"C_i\": Ci, \"L_thr\": Lthr,\n",
    "        \"g_iL\": g_iL, \"SINR\": SINR, \"rate\": rate\n",
    "    })\n",
    "    for k in range(K): df[f\"g_i{k}\"] = g_ik[:,k]\n",
    "\n",
    "    return {\n",
    "        \"users\": df,\n",
    "        \"aps_xy\": aps,\n",
    "        \"bs_xy\": bs,\n",
    "        \"noise\": noise\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581e90c",
   "metadata": {},
   "source": [
    "# Train Test Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7ecdf177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  x_LTE  ap_idx  subband   P_max_W       P_W       C_i     L_thr  \\\n",
      "0        0      0       4        9  0.069362  0.043642  1.298483  0.451962   \n",
      "1        1      0       1        5  0.153961  0.124950  3.646005  0.680345   \n",
      "2        2      0       4        6  0.171822  0.094295  5.294045  0.487091   \n",
      "3        3      0       0        8  0.167983  0.140216  4.062957  0.461594   \n",
      "4        4      0       3        4  0.117861  0.115636  1.323182  0.618151   \n",
      "\n",
      "           g_iL       SINR      rate          g_i0      g_i1      g_i2  \\\n",
      "0  2.401206e-05   1.562836  1.357741  1.777937e-05  0.000003  0.000004   \n",
      "1  1.294232e-05  24.728199  4.685279  1.184890e-07  0.002254  0.000023   \n",
      "2  4.236372e-07  77.753584  6.299274  1.920289e-03  0.000002  0.000006   \n",
      "3  4.277738e-06   1.749671  1.459259  1.858029e-04  0.000002  0.000003   \n",
      "4  1.927266e-04   6.078042  2.823350  2.709808e-05  0.000054  0.000299   \n",
      "\n",
      "       g_i3          g_i4          g_i5  \n",
      "0  0.000204  2.411748e-04  4.591673e-05  \n",
      "1  0.000002  4.626828e-07  4.752255e-07  \n",
      "2  0.000083  4.493138e-03  4.289480e-05  \n",
      "3  0.000007  1.533521e-05  6.864974e-06  \n",
      "4  0.000355  7.966613e-07  1.059690e-04  \n",
      "N users: 80 | K APs: 6 | F subbands: <module 'torch.nn.functional' from 'c:\\\\Users\\\\wine\\\\Giang\\\\Code\\\\Viet\\\\Cell-free-new\\\\D2D\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\functional.py'>\n"
     ]
    }
   ],
   "source": [
    "data_gen = generate_dataset(N=N, K=K, F=Fs, L=AREA, noise=NOISE_W,\n",
    "                            pmin=PMIN, pmax=PMAX, SL=SL, SW=SW, c=PRICE_C, seed=SEED)\n",
    "df = data_gen[\"users\"]\n",
    "print(df.head())\n",
    "print(\"N users:\", len(df), \"| K APs:\", K, \"| F subbands:\", F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32523b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df)\n",
    "g_L = df[\"g_iL\"].to_numpy(np.float32)\n",
    "P   = df[\"P_W\"].to_numpy(np.float32)\n",
    "ap  = df[\"ap_idx\"].to_numpy(np.int64)\n",
    "xL  = df[\"x_LTE\"].to_numpy(np.int64)\n",
    "sub = df[\"subband\"].to_numpy(np.int64)\n",
    "\n",
    "g_cols = [c for c in df.columns if c.startswith(\"g_i\") and c!=\"g_iL\"]\n",
    "g_cols_sorted = sorted(g_cols, key=lambda c: int(c.replace(\"g_i\",\"\")))\n",
    "g_ik = df[g_cols_sorted].to_numpy(np.float32) if g_cols_sorted else np.zeros((N,0), np.float32)\n",
    "K = g_ik.shape[1]\n",
    "noise = data_gen[\"noise\"]\n",
    "\n",
    "# hbar_ij: j -> máy thu của i\n",
    "hbar = np.zeros((N,N), dtype=np.float32)\n",
    "for i in range(N):\n",
    "    if xL[i]==1: hbar[i,:] = g_L[:]                             # j -> BS\n",
    "    else:\n",
    "        kstar = ap[i]\n",
    "        hbar[i,:] = g_ik[:, kstar] if 0 <= kstar < K else g_L[:]\n",
    "np.fill_diagonal(hbar, 0.0)\n",
    "\n",
    "# Cạnh có hướng giữa các user cùng băng con; trọng số ~ P_j * hbar_{ij}\n",
    "edge_src, edge_dst, edge_w = [], [], []\n",
    "scale = (P[:,None]*hbar).mean() + 1e-13\n",
    "\n",
    "for f in np.unique(sub):\n",
    "    ids = np.where(sub==f)[0]\n",
    "    if ids.size <= 1: \n",
    "        continue\n",
    "    for a in ids:\n",
    "        for b in ids:\n",
    "            if a == b: \n",
    "                continue\n",
    "            w = (P[b] * hbar[a,b]) / scale\n",
    "            # thưởng nhẹ nếu cùng điểm thu (cùng LTE hoặc cùng AP)\n",
    "            same_recv = (xL[a]==1 and xL[b]==1) or (ap[a]>=0 and ap[a]==ap[b]>=0)\n",
    "            if same_recv: \n",
    "                w += 0.1\n",
    "            edge_src.append(a); edge_dst.append(b); edge_w.append(float(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41295c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
    "edge_weight = torch.tensor(edge_w, dtype=torch.float32)\n",
    "\n",
    "# Features cho unsupervised: lấy mọi cột số an toàn (trừ id/meta)\n",
    "drop_cols = {\"user_id\"}\n",
    "feat_cols = [c for c in df.columns if c not in drop_cols]\n",
    "X_np = df[feat_cols].to_numpy(np.float32)\n",
    "\n",
    "# Chuẩn hoá feature (không có label)\n",
    "scaler = StandardScalerNP()\n",
    "X_np = scaler.fit_transform(X_np)\n",
    "\n",
    "x = torch.tensor(X_np, dtype=torch.float32)\n",
    "\n",
    "# Chia mask train/val/test (tùy hứng để quan sát loss/đánh giá phụ; unsupervised thực tế không bắt buộc)\n",
    "perm = rng.permutation(N)\n",
    "n_train = int(0.8 * N)\n",
    "train_mask = torch.zeros(N, dtype=torch.bool); train_mask[perm[:n_train]] = True\n",
    "val_mask   = torch.zeros(N, dtype=torch.bool); val_mask[perm[n_train:]] = True\n",
    "test_mask  = torch.zeros(N, dtype=torch.bool); test_mask[:] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf1787c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[80, 16], edge_index=[2, 640], edge_weight=[640], train_mask=[80], val_mask=[80], test_mask=[80])\n",
      "Edges: 640 | Feature dim: 16\n"
     ]
    }
   ],
   "source": [
    "data = Data(\n",
    "    x=x.to(device),\n",
    "    edge_index=edge_index.to(device),\n",
    "    edge_weight=edge_weight.to(device),\n",
    "    train_mask=train_mask.to(device),\n",
    "    val_mask=val_mask.to(device),\n",
    "    test_mask=test_mask.to(device)\n",
    ")\n",
    "print(data)\n",
    "print(f\"Edges: {edge_index.size(1)} | Feature dim: {data.x.size(-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480c292",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "77098396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden=128):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden)\n",
    "        self.conv2 = GCNConv(hidden, hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "060c0caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corruption(x, edge_index, edge_weight=None):\n",
    "    perm = torch.randperm(x.size(0), device=x.device)\n",
    "    x_cor = x[perm]\n",
    "    return x_cor, edge_index, edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6332cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = data.x.size(-1)\n",
    "HIDDEN = 128\n",
    "encoder = GCNEncoder(in_dim, hidden=HIDDEN).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "834e3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepGraphInfomax(\n",
    "    hidden_channels=HIDDEN,\n",
    "    encoder=encoder,\n",
    "    summary=lambda z, *args, **kwargs: torch.sigmoid(z.mean(dim=0)),\n",
    "    corruption=corruption\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3fc9c7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | DGI Loss 0.694165\n",
      "Epoch 002 | DGI Loss 0.678621\n",
      "Epoch 003 | DGI Loss 0.839534\n",
      "Epoch 004 | DGI Loss 0.745292\n",
      "Epoch 005 | DGI Loss 0.731227\n",
      "Epoch 006 | DGI Loss 0.768462\n",
      "Epoch 007 | DGI Loss 0.652986\n",
      "Epoch 008 | DGI Loss 0.663310\n",
      "Epoch 009 | DGI Loss 0.619599\n",
      "Epoch 010 | DGI Loss 0.693337\n",
      "Epoch 011 | DGI Loss 0.815674\n",
      "Epoch 012 | DGI Loss 0.757255\n",
      "Epoch 013 | DGI Loss 0.692985\n",
      "Epoch 014 | DGI Loss 0.718283\n",
      "Epoch 015 | DGI Loss 0.767184\n",
      "Epoch 016 | DGI Loss 0.778644\n",
      "Epoch 017 | DGI Loss 0.779975\n",
      "Epoch 018 | DGI Loss 0.838607\n",
      "Epoch 019 | DGI Loss 0.813146\n",
      "Epoch 020 | DGI Loss 1.050694\n",
      "Epoch 021 | DGI Loss 0.753130\n",
      "Epoch 022 | DGI Loss 0.897526\n",
      "Epoch 023 | DGI Loss 0.627854\n",
      "Epoch 024 | DGI Loss 0.809447\n",
      "Epoch 025 | DGI Loss 1.428406\n",
      "Epoch 026 | DGI Loss 0.927996\n",
      "Epoch 027 | DGI Loss 0.848322\n",
      "Epoch 028 | DGI Loss 0.888756\n",
      "Epoch 029 | DGI Loss 0.646029\n",
      "Epoch 030 | DGI Loss 0.820853\n",
      "Epoch 031 | DGI Loss 0.762035\n",
      "Epoch 032 | DGI Loss 0.880531\n",
      "Epoch 033 | DGI Loss 0.682202\n",
      "Epoch 034 | DGI Loss 0.793352\n",
      "Epoch 035 | DGI Loss 0.782209\n",
      "Epoch 036 | DGI Loss 0.837019\n",
      "Epoch 037 | DGI Loss 0.885286\n",
      "Epoch 038 | DGI Loss 0.844675\n",
      "Epoch 039 | DGI Loss 0.849995\n",
      "Epoch 040 | DGI Loss 1.048370\n",
      "Epoch 041 | DGI Loss 0.667731\n",
      "Epoch 042 | DGI Loss 0.728446\n",
      "Epoch 043 | DGI Loss 0.627514\n",
      "Epoch 044 | DGI Loss 0.684714\n",
      "Epoch 045 | DGI Loss 0.678699\n",
      "Epoch 046 | DGI Loss 0.847619\n",
      "Epoch 047 | DGI Loss 0.824636\n",
      "Epoch 048 | DGI Loss 0.687307\n",
      "Epoch 049 | DGI Loss 0.732298\n",
      "Epoch 050 | DGI Loss 0.733147\n",
      "Epoch 051 | DGI Loss 0.874182\n",
      "Epoch 052 | DGI Loss 0.818736\n",
      "Epoch 053 | DGI Loss 0.808986\n",
      "Epoch 054 | DGI Loss 0.605954\n",
      "Epoch 055 | DGI Loss 0.666265\n",
      "Epoch 056 | DGI Loss 0.810591\n",
      "Epoch 057 | DGI Loss 0.762313\n",
      "Epoch 058 | DGI Loss 0.763211\n",
      "Epoch 059 | DGI Loss 0.975662\n",
      "Epoch 060 | DGI Loss 0.789927\n",
      "Epoch 061 | DGI Loss 0.863228\n",
      "Epoch 062 | DGI Loss 0.826673\n",
      "Epoch 063 | DGI Loss 0.777846\n",
      "Epoch 064 | DGI Loss 0.741496\n",
      "Epoch 065 | DGI Loss 0.705198\n",
      "Epoch 066 | DGI Loss 0.641859\n",
      "Epoch 067 | DGI Loss 0.695005\n",
      "Epoch 068 | DGI Loss 0.800067\n",
      "Epoch 069 | DGI Loss 0.583888\n",
      "Epoch 070 | DGI Loss 0.646610\n",
      "Epoch 071 | DGI Loss 0.585649\n",
      "Epoch 072 | DGI Loss 0.782130\n",
      "Epoch 073 | DGI Loss 0.726281\n",
      "Epoch 074 | DGI Loss 0.554308\n",
      "Epoch 075 | DGI Loss 0.544136\n",
      "Epoch 076 | DGI Loss 0.792982\n",
      "Epoch 077 | DGI Loss 0.900289\n",
      "Epoch 078 | DGI Loss 0.751292\n",
      "Epoch 079 | DGI Loss 0.681292\n",
      "Epoch 080 | DGI Loss 0.783988\n",
      "Epoch 081 | DGI Loss 0.566298\n",
      "Epoch 082 | DGI Loss 0.633689\n",
      "Epoch 083 | DGI Loss 0.632387\n",
      "Epoch 084 | DGI Loss 0.536492\n",
      "Epoch 085 | DGI Loss 0.686908\n",
      "Epoch 086 | DGI Loss 0.617525\n",
      "Epoch 087 | DGI Loss 0.590865\n",
      "Epoch 088 | DGI Loss 0.632761\n",
      "Epoch 089 | DGI Loss 0.695528\n",
      "Epoch 090 | DGI Loss 0.514962\n",
      "Epoch 091 | DGI Loss 0.559668\n",
      "Epoch 092 | DGI Loss 0.592612\n",
      "Epoch 093 | DGI Loss 0.625634\n",
      "Epoch 094 | DGI Loss 0.585963\n",
      "Epoch 095 | DGI Loss 0.573622\n",
      "Epoch 096 | DGI Loss 0.675455\n",
      "Epoch 097 | DGI Loss 0.579018\n",
      "Epoch 098 | DGI Loss 0.605237\n",
      "Epoch 099 | DGI Loss 0.669567\n",
      "Epoch 100 | DGI Loss 0.582677\n",
      "Epoch 101 | DGI Loss 0.521063\n",
      "Epoch 102 | DGI Loss 0.584051\n",
      "Epoch 103 | DGI Loss 0.655264\n",
      "Epoch 104 | DGI Loss 0.524359\n",
      "Epoch 105 | DGI Loss 0.596608\n",
      "Epoch 106 | DGI Loss 0.762218\n",
      "Epoch 107 | DGI Loss 0.466373\n",
      "Epoch 108 | DGI Loss 0.587764\n",
      "Epoch 109 | DGI Loss 0.598003\n",
      "Epoch 110 | DGI Loss 0.633245\n",
      "Epoch 111 | DGI Loss 0.679025\n",
      "Epoch 112 | DGI Loss 0.629996\n",
      "Epoch 113 | DGI Loss 0.493765\n",
      "Epoch 114 | DGI Loss 0.784837\n",
      "Epoch 115 | DGI Loss 0.532547\n",
      "Epoch 116 | DGI Loss 0.608361\n",
      "Epoch 117 | DGI Loss 0.622396\n",
      "Epoch 118 | DGI Loss 0.654781\n",
      "Epoch 119 | DGI Loss 0.604556\n",
      "Epoch 120 | DGI Loss 0.549503\n",
      "Epoch 121 | DGI Loss 0.630697\n",
      "Epoch 122 | DGI Loss 0.568958\n",
      "Epoch 123 | DGI Loss 0.532426\n",
      "Epoch 124 | DGI Loss 0.622438\n",
      "Epoch 125 | DGI Loss 0.584471\n",
      "Epoch 126 | DGI Loss 0.636063\n",
      "Epoch 127 | DGI Loss 0.425846\n",
      "Epoch 128 | DGI Loss 0.626240\n",
      "Epoch 129 | DGI Loss 0.578306\n",
      "Epoch 130 | DGI Loss 0.551811\n",
      "Epoch 131 | DGI Loss 0.490101\n",
      "Epoch 132 | DGI Loss 0.520482\n",
      "Epoch 133 | DGI Loss 0.563362\n",
      "Epoch 134 | DGI Loss 0.567899\n",
      "Epoch 135 | DGI Loss 0.494465\n",
      "Epoch 136 | DGI Loss 0.738865\n",
      "Epoch 137 | DGI Loss 0.647564\n",
      "Epoch 138 | DGI Loss 0.500866\n",
      "Epoch 139 | DGI Loss 0.679325\n",
      "Epoch 140 | DGI Loss 0.675953\n",
      "Epoch 141 | DGI Loss 0.892483\n",
      "Epoch 142 | DGI Loss 0.793152\n",
      "Epoch 143 | DGI Loss 0.999657\n",
      "Epoch 144 | DGI Loss 0.734169\n",
      "Epoch 145 | DGI Loss 0.486838\n",
      "Epoch 146 | DGI Loss 0.804423\n",
      "Epoch 147 | DGI Loss 0.692501\n",
      "Epoch 148 | DGI Loss 0.504543\n",
      "Epoch 149 | DGI Loss 0.613759\n",
      "Epoch 150 | DGI Loss 0.729658\n",
      "Epoch 151 | DGI Loss 0.623423\n",
      "Epoch 152 | DGI Loss 0.549898\n",
      "Epoch 153 | DGI Loss 0.641309\n",
      "Epoch 154 | DGI Loss 0.629704\n",
      "Epoch 155 | DGI Loss 0.629610\n",
      "Epoch 156 | DGI Loss 0.770844\n",
      "Epoch 157 | DGI Loss 0.579811\n",
      "Epoch 158 | DGI Loss 0.588133\n",
      "Epoch 159 | DGI Loss 0.722484\n",
      "Epoch 160 | DGI Loss 0.618991\n",
      "Epoch 161 | DGI Loss 0.641295\n",
      "Epoch 162 | DGI Loss 0.570141\n",
      "Epoch 163 | DGI Loss 0.656655\n",
      "Epoch 164 | DGI Loss 0.674234\n",
      "Epoch 165 | DGI Loss 0.652126\n",
      "Epoch 166 | DGI Loss 0.759094\n",
      "Epoch 167 | DGI Loss 0.877014\n",
      "Epoch 168 | DGI Loss 0.563602\n",
      "Epoch 169 | DGI Loss 0.573263\n",
      "Epoch 170 | DGI Loss 0.657948\n",
      "Epoch 171 | DGI Loss 0.652611\n",
      "Epoch 172 | DGI Loss 0.648747\n",
      "Epoch 173 | DGI Loss 0.571105\n",
      "Epoch 174 | DGI Loss 0.526955\n",
      "Epoch 175 | DGI Loss 0.655092\n",
      "Epoch 176 | DGI Loss 0.492988\n",
      "Epoch 177 | DGI Loss 0.531079\n",
      "Epoch 178 | DGI Loss 0.486420\n",
      "Epoch 179 | DGI Loss 0.425033\n",
      "Epoch 180 | DGI Loss 0.396489\n",
      "Epoch 181 | DGI Loss 0.546708\n",
      "Epoch 182 | DGI Loss 0.546000\n",
      "Epoch 183 | DGI Loss 0.525526\n",
      "Epoch 184 | DGI Loss 0.390834\n",
      "Epoch 185 | DGI Loss 0.673559\n",
      "Epoch 186 | DGI Loss 0.565170\n",
      "Epoch 187 | DGI Loss 0.653434\n",
      "Epoch 188 | DGI Loss 0.520984\n",
      "Epoch 189 | DGI Loss 0.750260\n",
      "Epoch 190 | DGI Loss 0.465997\n",
      "Epoch 191 | DGI Loss 0.525213\n",
      "Epoch 192 | DGI Loss 0.677215\n",
      "Epoch 193 | DGI Loss 0.397487\n",
      "Epoch 194 | DGI Loss 0.553485\n",
      "Epoch 195 | DGI Loss 0.691417\n",
      "Epoch 196 | DGI Loss 0.818704\n",
      "Epoch 197 | DGI Loss 0.520994\n",
      "Epoch 198 | DGI Loss 0.543187\n",
      "Epoch 199 | DGI Loss 0.685067\n",
      "Epoch 200 | DGI Loss 0.516816\n",
      "Epoch 201 | DGI Loss 0.727004\n",
      "Epoch 202 | DGI Loss 0.370936\n",
      "Epoch 203 | DGI Loss 0.494372\n",
      "Epoch 204 | DGI Loss 0.527057\n",
      "Epoch 205 | DGI Loss 0.706775\n",
      "Epoch 206 | DGI Loss 0.645304\n",
      "Epoch 207 | DGI Loss 0.605728\n",
      "Epoch 208 | DGI Loss 0.551721\n",
      "Epoch 209 | DGI Loss 0.674756\n",
      "Epoch 210 | DGI Loss 0.605339\n",
      "Epoch 211 | DGI Loss 0.517754\n",
      "Epoch 212 | DGI Loss 0.783529\n",
      "Epoch 213 | DGI Loss 0.804534\n",
      "Epoch 214 | DGI Loss 0.827918\n",
      "Epoch 215 | DGI Loss 0.736424\n",
      "Epoch 216 | DGI Loss 0.661031\n",
      "Epoch 217 | DGI Loss 0.535644\n",
      "Epoch 218 | DGI Loss 0.576056\n",
      "Epoch 219 | DGI Loss 0.635273\n",
      "Epoch 220 | DGI Loss 0.479282\n",
      "Epoch 221 | DGI Loss 0.437439\n",
      "Epoch 222 | DGI Loss 0.595688\n",
      "Epoch 223 | DGI Loss 0.487700\n",
      "Epoch 224 | DGI Loss 0.517629\n",
      "Epoch 225 | DGI Loss 0.663342\n",
      "Epoch 226 | DGI Loss 0.505220\n",
      "Epoch 227 | DGI Loss 0.433282\n",
      "Epoch 228 | DGI Loss 0.493209\n",
      "Epoch 229 | DGI Loss 0.689004\n",
      "Epoch 230 | DGI Loss 0.692634\n",
      "Epoch 231 | DGI Loss 0.554428\n",
      "Epoch 232 | DGI Loss 0.571205\n",
      "Epoch 233 | DGI Loss 0.534532\n",
      "Epoch 234 | DGI Loss 0.478221\n",
      "Epoch 235 | DGI Loss 0.468433\n",
      "Epoch 236 | DGI Loss 0.715643\n",
      "Epoch 237 | DGI Loss 0.524888\n",
      "Epoch 238 | DGI Loss 0.515734\n",
      "Epoch 239 | DGI Loss 0.488884\n",
      "Epoch 240 | DGI Loss 0.535269\n",
      "Epoch 241 | DGI Loss 0.563150\n",
      "Epoch 242 | DGI Loss 0.642157\n",
      "Epoch 243 | DGI Loss 0.513797\n",
      "Epoch 244 | DGI Loss 0.550643\n",
      "Epoch 245 | DGI Loss 0.703850\n",
      "Epoch 246 | DGI Loss 0.695162\n",
      "Epoch 247 | DGI Loss 0.627482\n",
      "Epoch 248 | DGI Loss 1.015304\n",
      "Epoch 249 | DGI Loss 0.751135\n",
      "Epoch 250 | DGI Loss 0.577846\n",
      "Epoch 251 | DGI Loss 0.606411\n",
      "Epoch 252 | DGI Loss 0.667567\n",
      "Epoch 253 | DGI Loss 0.548937\n",
      "Epoch 254 | DGI Loss 0.511324\n",
      "Epoch 255 | DGI Loss 0.610026\n",
      "Epoch 256 | DGI Loss 0.494075\n",
      "Epoch 257 | DGI Loss 0.609200\n",
      "Epoch 258 | DGI Loss 0.763487\n",
      "Epoch 259 | DGI Loss 1.067725\n",
      "Epoch 260 | DGI Loss 0.679459\n",
      "Epoch 261 | DGI Loss 0.646884\n",
      "Epoch 262 | DGI Loss 0.558648\n",
      "Epoch 263 | DGI Loss 0.568649\n",
      "Epoch 264 | DGI Loss 0.592906\n",
      "Epoch 265 | DGI Loss 0.523774\n",
      "Epoch 266 | DGI Loss 0.777424\n",
      "Epoch 267 | DGI Loss 0.529208\n",
      "Epoch 268 | DGI Loss 0.736515\n",
      "Epoch 269 | DGI Loss 0.557200\n",
      "Epoch 270 | DGI Loss 0.545178\n",
      "Epoch 271 | DGI Loss 0.392254\n",
      "Epoch 272 | DGI Loss 0.546092\n",
      "Epoch 273 | DGI Loss 0.524764\n",
      "Epoch 274 | DGI Loss 0.506656\n",
      "Epoch 275 | DGI Loss 0.724417\n",
      "Epoch 276 | DGI Loss 0.667876\n",
      "Epoch 277 | DGI Loss 0.406524\n",
      "Epoch 278 | DGI Loss 0.539607\n",
      "Epoch 279 | DGI Loss 0.416015\n",
      "Epoch 280 | DGI Loss 0.606581\n",
      "Epoch 281 | DGI Loss 0.441527\n",
      "Epoch 282 | DGI Loss 0.677411\n",
      "Epoch 283 | DGI Loss 0.450862\n",
      "Epoch 284 | DGI Loss 0.453804\n",
      "Epoch 285 | DGI Loss 0.517591\n",
      "Epoch 286 | DGI Loss 0.569102\n",
      "Epoch 287 | DGI Loss 0.508106\n",
      "Epoch 288 | DGI Loss 0.466779\n",
      "Epoch 289 | DGI Loss 0.397248\n",
      "Epoch 290 | DGI Loss 0.533424\n",
      "Epoch 291 | DGI Loss 0.547336\n",
      "Epoch 292 | DGI Loss 0.604019\n",
      "Epoch 293 | DGI Loss 0.433762\n",
      "Epoch 294 | DGI Loss 0.593823\n",
      "Epoch 295 | DGI Loss 0.671104\n",
      "Epoch 296 | DGI Loss 0.638422\n",
      "Epoch 297 | DGI Loss 0.625911\n",
      "Epoch 298 | DGI Loss 0.533800\n",
      "Epoch 299 | DGI Loss 0.543376\n",
      "Epoch 300 | DGI Loss 0.606689\n",
      "Epoch 301 | DGI Loss 0.601424\n",
      "Epoch 302 | DGI Loss 0.502322\n",
      "Epoch 303 | DGI Loss 0.421903\n",
      "Epoch 304 | DGI Loss 0.551873\n",
      "Epoch 305 | DGI Loss 0.710708\n",
      "Epoch 306 | DGI Loss 0.624513\n",
      "Epoch 307 | DGI Loss 0.521369\n",
      "Epoch 308 | DGI Loss 0.668453\n",
      "Epoch 309 | DGI Loss 0.569672\n",
      "Epoch 310 | DGI Loss 0.570695\n",
      "Epoch 311 | DGI Loss 0.541166\n",
      "Epoch 312 | DGI Loss 0.652988\n",
      "Epoch 313 | DGI Loss 0.662941\n",
      "Epoch 314 | DGI Loss 0.625171\n",
      "Epoch 315 | DGI Loss 0.430885\n",
      "Epoch 316 | DGI Loss 0.452188\n",
      "Epoch 317 | DGI Loss 0.721571\n",
      "Epoch 318 | DGI Loss 0.491205\n",
      "Epoch 319 | DGI Loss 0.582613\n",
      "Epoch 320 | DGI Loss 0.560614\n",
      "Epoch 321 | DGI Loss 0.567614\n",
      "Epoch 322 | DGI Loss 0.481314\n",
      "Epoch 323 | DGI Loss 0.417296\n",
      "Epoch 324 | DGI Loss 0.566913\n",
      "Epoch 325 | DGI Loss 0.418126\n",
      "Epoch 326 | DGI Loss 0.439359\n",
      "Epoch 327 | DGI Loss 0.534357\n",
      "Epoch 328 | DGI Loss 0.462504\n",
      "Epoch 329 | DGI Loss 0.483315\n",
      "Epoch 330 | DGI Loss 0.386013\n",
      "Epoch 331 | DGI Loss 0.606057\n",
      "Epoch 332 | DGI Loss 0.493913\n",
      "Epoch 333 | DGI Loss 0.618519\n",
      "Epoch 334 | DGI Loss 0.618439\n",
      "Epoch 335 | DGI Loss 0.433208\n",
      "Epoch 336 | DGI Loss 0.494020\n",
      "Epoch 337 | DGI Loss 0.610763\n",
      "Epoch 338 | DGI Loss 0.658646\n",
      "Epoch 339 | DGI Loss 0.569318\n",
      "Epoch 340 | DGI Loss 0.531453\n",
      "Epoch 341 | DGI Loss 0.512171\n",
      "Epoch 342 | DGI Loss 0.683032\n",
      "Epoch 343 | DGI Loss 0.665733\n",
      "Epoch 344 | DGI Loss 0.619249\n",
      "Epoch 345 | DGI Loss 0.609329\n",
      "Epoch 346 | DGI Loss 0.611358\n",
      "Epoch 347 | DGI Loss 0.625990\n",
      "Epoch 348 | DGI Loss 0.660689\n",
      "Epoch 349 | DGI Loss 0.563385\n",
      "Epoch 350 | DGI Loss 0.627614\n",
      "Epoch 351 | DGI Loss 0.501639\n",
      "Epoch 352 | DGI Loss 0.492058\n",
      "Epoch 353 | DGI Loss 0.756805\n",
      "Epoch 354 | DGI Loss 0.728757\n",
      "Epoch 355 | DGI Loss 0.646753\n",
      "Epoch 356 | DGI Loss 0.715322\n",
      "Epoch 357 | DGI Loss 0.583095\n",
      "Epoch 358 | DGI Loss 0.546687\n",
      "Epoch 359 | DGI Loss 0.604638\n",
      "Epoch 360 | DGI Loss 0.759078\n",
      "Epoch 361 | DGI Loss 0.774537\n",
      "Epoch 362 | DGI Loss 0.511337\n",
      "Epoch 363 | DGI Loss 0.645059\n",
      "Epoch 364 | DGI Loss 0.460453\n",
      "Epoch 365 | DGI Loss 0.552255\n",
      "Epoch 366 | DGI Loss 0.572098\n",
      "Epoch 367 | DGI Loss 0.421679\n",
      "Epoch 368 | DGI Loss 0.425793\n",
      "Epoch 369 | DGI Loss 0.836377\n",
      "Epoch 370 | DGI Loss 0.547513\n",
      "Epoch 371 | DGI Loss 0.387156\n",
      "Epoch 372 | DGI Loss 0.569450\n",
      "Epoch 373 | DGI Loss 0.704320\n",
      "Epoch 374 | DGI Loss 0.526537\n",
      "Epoch 375 | DGI Loss 0.374122\n",
      "Epoch 376 | DGI Loss 0.488404\n",
      "Epoch 377 | DGI Loss 0.623167\n",
      "Epoch 378 | DGI Loss 0.553114\n",
      "Epoch 379 | DGI Loss 0.671990\n",
      "Epoch 380 | DGI Loss 0.479163\n",
      "Epoch 381 | DGI Loss 0.733434\n",
      "Epoch 382 | DGI Loss 0.505511\n",
      "Epoch 383 | DGI Loss 0.603238\n",
      "Epoch 384 | DGI Loss 0.507038\n",
      "Epoch 385 | DGI Loss 0.500014\n",
      "Epoch 386 | DGI Loss 0.489844\n",
      "Epoch 387 | DGI Loss 0.464439\n",
      "Epoch 388 | DGI Loss 0.474137\n",
      "Epoch 389 | DGI Loss 0.503932\n",
      "Epoch 390 | DGI Loss 0.544497\n",
      "Epoch 391 | DGI Loss 0.573826\n",
      "Epoch 392 | DGI Loss 0.536333\n",
      "Epoch 393 | DGI Loss 0.450641\n",
      "Epoch 394 | DGI Loss 0.401929\n",
      "Epoch 395 | DGI Loss 0.588537\n",
      "Epoch 396 | DGI Loss 0.390754\n",
      "Epoch 397 | DGI Loss 0.503273\n",
      "Epoch 398 | DGI Loss 0.699540\n",
      "Epoch 399 | DGI Loss 0.432803\n",
      "Epoch 400 | DGI Loss 0.416473\n",
      "Epoch 401 | DGI Loss 0.503767\n",
      "Epoch 402 | DGI Loss 0.557862\n",
      "Epoch 403 | DGI Loss 0.586731\n",
      "Epoch 404 | DGI Loss 0.529150\n",
      "Epoch 405 | DGI Loss 0.471130\n",
      "Epoch 406 | DGI Loss 0.472983\n",
      "Epoch 407 | DGI Loss 0.510794\n",
      "Epoch 408 | DGI Loss 0.523066\n",
      "Epoch 409 | DGI Loss 0.440855\n",
      "Epoch 410 | DGI Loss 0.559897\n",
      "Epoch 411 | DGI Loss 0.351661\n",
      "Epoch 412 | DGI Loss 0.406332\n",
      "Epoch 413 | DGI Loss 0.472020\n",
      "Epoch 414 | DGI Loss 0.329459\n",
      "Epoch 415 | DGI Loss 0.652450\n",
      "Epoch 416 | DGI Loss 0.426639\n",
      "Epoch 417 | DGI Loss 0.361790\n",
      "Epoch 418 | DGI Loss 0.461738\n",
      "Epoch 419 | DGI Loss 0.442499\n",
      "Epoch 420 | DGI Loss 0.442103\n",
      "Epoch 421 | DGI Loss 0.439064\n",
      "Epoch 422 | DGI Loss 0.472297\n",
      "Epoch 423 | DGI Loss 0.459954\n",
      "Epoch 424 | DGI Loss 0.494760\n",
      "Epoch 425 | DGI Loss 0.459010\n",
      "Epoch 426 | DGI Loss 0.362348\n",
      "Epoch 427 | DGI Loss 0.368381\n",
      "Epoch 428 | DGI Loss 0.328164\n",
      "Epoch 429 | DGI Loss 0.563228\n",
      "Epoch 430 | DGI Loss 0.471478\n",
      "Epoch 431 | DGI Loss 0.423585\n",
      "Epoch 432 | DGI Loss 0.441374\n",
      "Epoch 433 | DGI Loss 0.328941\n",
      "Epoch 434 | DGI Loss 0.391863\n",
      "Epoch 435 | DGI Loss 0.554875\n",
      "Epoch 436 | DGI Loss 0.521125\n",
      "Epoch 437 | DGI Loss 0.329732\n",
      "Epoch 438 | DGI Loss 0.370460\n",
      "Epoch 439 | DGI Loss 0.388784\n",
      "Epoch 440 | DGI Loss 0.410703\n",
      "Epoch 441 | DGI Loss 0.505703\n",
      "Epoch 442 | DGI Loss 0.575546\n",
      "Epoch 443 | DGI Loss 0.475628\n",
      "Epoch 444 | DGI Loss 0.545880\n",
      "Epoch 445 | DGI Loss 0.531912\n",
      "Epoch 446 | DGI Loss 0.818235\n",
      "Epoch 447 | DGI Loss 0.610872\n",
      "Epoch 448 | DGI Loss 0.713408\n",
      "Epoch 449 | DGI Loss 0.676424\n",
      "Epoch 450 | DGI Loss 0.642968\n",
      "Epoch 451 | DGI Loss 0.510303\n",
      "Epoch 452 | DGI Loss 0.685873\n",
      "Epoch 453 | DGI Loss 0.866934\n",
      "Epoch 454 | DGI Loss 0.547094\n",
      "Epoch 455 | DGI Loss 0.665171\n",
      "Epoch 456 | DGI Loss 0.408625\n",
      "Epoch 457 | DGI Loss 0.729940\n",
      "Epoch 458 | DGI Loss 0.563752\n",
      "Epoch 459 | DGI Loss 0.630306\n",
      "Epoch 460 | DGI Loss 0.695909\n",
      "Epoch 461 | DGI Loss 0.308077\n",
      "Epoch 462 | DGI Loss 0.476459\n",
      "Epoch 463 | DGI Loss 0.651405\n",
      "Epoch 464 | DGI Loss 0.663634\n",
      "Epoch 465 | DGI Loss 0.644234\n",
      "Epoch 466 | DGI Loss 0.485016\n",
      "Epoch 467 | DGI Loss 0.690282\n",
      "Epoch 468 | DGI Loss 0.509825\n",
      "Epoch 469 | DGI Loss 0.559459\n",
      "Epoch 470 | DGI Loss 0.477383\n",
      "Epoch 471 | DGI Loss 0.544878\n",
      "Epoch 472 | DGI Loss 0.634776\n",
      "Epoch 473 | DGI Loss 0.567949\n",
      "Epoch 474 | DGI Loss 0.510124\n",
      "Epoch 475 | DGI Loss 0.579292\n",
      "Epoch 476 | DGI Loss 0.650213\n",
      "Epoch 477 | DGI Loss 0.473423\n",
      "Epoch 478 | DGI Loss 0.687500\n",
      "Epoch 479 | DGI Loss 0.603474\n",
      "Epoch 480 | DGI Loss 0.449293\n",
      "Epoch 481 | DGI Loss 0.425662\n",
      "Epoch 482 | DGI Loss 0.679200\n",
      "Epoch 483 | DGI Loss 0.508066\n",
      "Epoch 484 | DGI Loss 0.457300\n",
      "Epoch 485 | DGI Loss 0.560135\n",
      "Epoch 486 | DGI Loss 0.653080\n",
      "Epoch 487 | DGI Loss 0.507335\n",
      "Epoch 488 | DGI Loss 0.509027\n",
      "Epoch 489 | DGI Loss 0.595090\n",
      "Epoch 490 | DGI Loss 0.396965\n",
      "Epoch 491 | DGI Loss 0.586328\n",
      "Epoch 492 | DGI Loss 0.535920\n",
      "Epoch 493 | DGI Loss 0.630297\n",
      "Epoch 494 | DGI Loss 0.578834\n",
      "Epoch 495 | DGI Loss 0.559748\n",
      "Epoch 496 | DGI Loss 0.599437\n",
      "Epoch 497 | DGI Loss 0.447768\n",
      "Epoch 498 | DGI Loss 0.741654\n",
      "Epoch 499 | DGI Loss 0.425062\n",
      "Epoch 500 | DGI Loss 0.457950\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "logs = []\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pos_z, neg_z, summary = model(data.x, data.edge_index, edge_weight=data.edge_weight)\n",
    "    loss = model.loss(pos_z, neg_z, summary)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # log mỗi epoch\n",
    "    print(f\"Epoch {epoch:03d} | DGI Loss {loss.item():.6f}\")\n",
    "    logs.append({\"epoch\": epoch, \"dgi_loss\": float(loss.item())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3c3f5d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (80, 128)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Z = model.encoder(data.x, data.edge_index, edge_weight=data.edge_weight)  # (N, HIDDEN)\n",
    "print(\"Embeddings shape:\", tuple(Z.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
